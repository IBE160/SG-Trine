# Epic Technical Specification: AI-Powered Task Intelligence

Date: 2025-11-30
Author: BIP
Epic ID: epic-2
Status: Draft

---

## Overview

This epic, "AI-Powered Task Intelligence," marks the first major integration of AI capabilities into the "Prioritize" application. The primary goal is to enhance task clarity and reduce user cognitive load by automatically analyzing tasks to provide smart categorization and prioritization. This directly addresses the core product vision of transforming a simple to-do list into an intelligent, proactive assistant. This epic covers functional requirements FR8, FR9, and FR10.

## Objectives and Scope

**In Scope:**

*   **FR8: AI-Generated Smart Labels:** The system will asynchronously call the Gemini API from the backend to generate relevant labels (e.g., "work," "personal," "urgent") for each new or updated task. These labels will be stored and associated with the tasks in the database.
*   **FR9: AI-Suggested Priority Levels:** The backend will similarly call the Gemini API to analyze a task's content and suggest a priority level (e.g., 1-4). This priority will be stored on the task record.
*   **FR10: Filter by Smart Label:** The frontend UI will be updated to display the generated labels on each task. Users will be able to click on a label to filter their task list, showing only tasks with that specific label.
*   **Backend API Extension:** The `/api/v1/tasks` endpoint will be modified to accept a `label` query parameter to support filtering.
*   **Database Schema Implementation:** The `labels` and `task_labels` tables will be created and integrated as defined in the architecture document to support the many-to-many relationship between tasks and labels.

**Out of Scope:**

*   Any AI-powered features from other epics, such as task breakdown (Epic 4), time estimates (Epic 4), or the "Just Start Here" button (Epic 4).
*   User interface changes beyond what is necessary to display labels and provide filtering controls.
*   The ability for users to create, edit, or delete labels manually. For this epic, labels are exclusively generated by the AI.

## System Architecture Alignment

This epic's implementation will strictly adhere to the established architectural patterns to ensure consistency and maintainability.

*   **AI Integration Strategy:** All communication with the Gemini 2.5 Pro API will be handled exclusively by the **FastAPI backend**, as mandated by the architecture (Section 4.3). This protects the API key and centralizes prompt engineering, error handling, and fallback logic.
*   **Database Schema:** The implementation will create the `labels` and `task_labels` tables as defined in the database schema (Architecture, Section 4.1), establishing the required many-to-many relationship.
*   **API Design:** The enhancement of the `GET /api/v1/tasks` endpoint to include filtering capabilities follows the versioned RESTful API design principles (Architecture, Section 4.4).
*   **Frontend/Backend Communication:** The Next.js frontend will consume the updated API, sending requests and handling responses via the established type-safe API client.

## Detailed Design

### Services and Modules

| Service / Module | Location | Responsibilities | Inputs | Outputs |
| :--- | :--- | :--- | :--- | :--- |
| **TaskService** | Backend (FastAPI) | Orchestrates CRUD operations for tasks. Triggers asynchronous AI analysis upon task creation/update. Contains logic for filtering tasks by label. | User ID, Task data (from API) | Task objects, Lists of tasks |
| **AIService** | Backend (FastAPI) | Constructs prompts for Gemini API. Manages calls to Gemini for label and priority generation. Parses responses and handles errors/fallbacks. | Task content (title, description) | Suggested labels, Priority level |
| **TaskRepository** | Backend (FastAPI) | Handles all database interactions related to tasks, labels, and their relationships. Encapsulates all SQL queries. | Task/Label data | Database records |
| **TaskStateService** | Frontend (Zustand) | Manages the global state of the task list in the UI. Fetches and caches tasks from the backend. Triggers API calls for filtering. | Filter criteria (e.g., label) | Task list for UI components |
| **LabelTag** | Frontend (React) | A UI component that displays a single label and handles the click event to trigger filtering. | Label name | Click event |

### Data Models and Contracts

The following data model changes are required to support this epic.

**1. Database Schema (SQL)**

A `priority` column will be added to the `tasks` table. Two new tables, `labels` and `task_labels`, will be created to manage the many-to-many relationship.

```sql
-- Add priority to tasks table
ALTER TABLE tasks ADD COLUMN priority INTEGER;

-- Create labels table for unique label names
CREATE TABLE labels (
    id BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
    name TEXT NOT NULL UNIQUE,
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- Create task_labels join table
CREATE TABLE task_labels (
    task_id BIGINT REFERENCES tasks(id) ON DELETE CASCADE,
    label_id BIGINT REFERENCES labels(id) ON DELETE CASCADE,
    PRIMARY KEY (task_id, label_id)
);
```

**2. API Data Contracts (Pydantic)**

The `TaskRead` model returned by the API will be updated to include the new fields.

```python
from typing import List, Optional
from pydantic import BaseModel

class TaskRead(BaseModel):
    id: int
    title: str
    is_completed: bool
    # ... other existing fields ...
    priority: Optional[int]
    labels: List[str] = []

    class Config:
        orm_mode = True
```

### APIs and Interfaces

The primary API change is the enhancement of the `GET /api/v1/tasks` endpoint.

**`GET /api/v1/tasks`**

*   **Description:** Fetches tasks for the authenticated user. Now supports filtering by a smart label.
*   **Query Parameters:**
    *   `label` (string, optional): If provided, the API will only return tasks that have this specific label.
*   **Success Response (200 OK):**
    *   **Body:** `List[TaskRead]`
    *   **Description:** Returns a list of task objects. The `labels` property of each task object will be populated with a list of its associated label names.

### Workflows and Sequencing

**1. AI-Powered Task Creation/Update Workflow**

This sequence details how a task is analyzed asynchronously.

```mermaid
sequenceDiagram
    participant User
    participant Frontend (Next.js)
    participant Backend (FastAPI)
    participant AIService
    participant Gemini API
    participant Database (Supabase)

    User->>Frontend (Next.js): Creates/Edits a task
    Frontend (Next.js)->>Backend (FastAPI): POST /api/v1/tasks (task data)
    Backend (FastAPI)->>Database (Supabase): INSERT/UPDATE tasks table
    Database (Supabase)-->>Backend (FastAPI): Return saved task with ID
    Backend (FastAPI)-->>Frontend (Next.js): 201 Created (task without labels/priority yet)

    par Asynchronous AI Analysis
        Backend (FastAPI)->>AIService: AnalyzeTask(task_content)
        AIService->>Gemini API: Generate labels and priority for task
        Gemini API-->>AIService: Return suggestions {labels: ['...'], priority: 2}
        AIService->>Database (Supabase): 1. Find or create labels in 'labels' table
        AIService->>Database (Supabase): 2. Create entries in 'task_labels' join table
        AIService->>Database (Supabase): 3. UPDATE priority on 'tasks' table
    end

    Note over Frontend (Next.js), Database (Supabase): Frontend receives real-time update via Supabase subscription, displaying new labels and priority.
```

**2. Task Filtering Workflow**

This sequence shows how the UI updates when a user filters the list.

```mermaid
sequenceDiagram
    participant User
    participant Frontend (Next.js)
    participant Backend (FastAPI)
    participant Database (Supabase)

    User->>Frontend (Next.js): Clicks on a 'work' label tag
    Frontend (Next.js)->>Backend (FastAPI): GET /api/v1/tasks?label=work
    Backend (FastAPI)->>Database (Supabase): Query tasks JOIN task_labels, labels WHERE label.name = 'work'
    Database (Supabase)-->>Backend (FastAPI): Return filtered list of tasks
    Backend (FastAPI)-->>Frontend (Next.js): 200 OK (List of tasks with 'work' label)
    Frontend (Next.js)->>User: Update UI to display only filtered tasks
```

## Non-Functional Requirements

### Performance

*   **Asynchronous AI Calls:** Calls to the Gemini API for label and priority generation **MUST** be performed asynchronously (e.g., using FastAPI's `BackgroundTasks`) to ensure the initial task creation/update API response is not blocked. The user should see their task appear instantly in the UI.
*   **Database Indexing:** To ensure efficient filtering, indexes must be created on the `labels.name` column and the foreign key columns in the `task_labels` join table.
*   **API Latency:** The `GET /api/v1/tasks` endpoint, including filtering, should respond in under 200ms on average.

### Security

*   **API Key Management:** The Gemini API key **MUST** be stored as a secure environment variable on the backend and managed through Vercel's project settings. It must never be exposed to the frontend.
*   **Data Isolation:** All database queries within the `TaskService` and `TaskRepository` **MUST** be scoped to the authenticated user's ID, leveraging the established RLS policies. This ensures the AI is only triggered for and acts upon data owned by the user making the request.

### Reliability/Availability

*   **AI Service Fallback:** The `AIService` **MUST** implement robust error handling for calls to the Gemini API. If the API call fails, times out, or returns a malformed response, the error must be logged, but the core task creation/update process must not fail. The task will simply exist without AI-generated attributes for the time being.
*   **Idempotency:** The logic for creating labels should be idempotent. If the same label is suggested multiple times, it should not create duplicate entries in the `labels` table.

### Observability

*   **AI Interaction Logging:** All interactions with the Gemini API **MUST** be logged. This includes the full prompt sent to the API and the complete, raw response received. This is critical for auditing, debugging, and refining prompts.
*   **Error Logging:** Any errors encountered within the `AIService` (e.g., API errors, parsing failures, database errors) must be logged with a high severity level.
*   **Performance Monitoring:** The latency of Gemini API calls should be tracked and monitored to identify potential performance degradation over time. This can be done by adding custom metrics.

## Dependencies and Integrations

This epic introduces a new core integration with the Gemini API and updates existing dependencies to support new features.

### Backend Dependencies (FastAPI - `requirements.txt`)

*   **`google-generativeai`**: (New) Essential for programmatic access to the Gemini 2.5 Pro API for generating smart labels and priority levels.
*   **`fastapi`**: Core framework for building the backend API.
*   **`uvicorn`**: ASGI server to run the FastAPI application.
*   **`supabase-py`**: Client library for interacting with the Supabase database.
*   **`pydantic`**: For data validation and settings management.
*   **`pydantic-settings`**: For managing application settings, including API keys.

### Frontend Dependencies (Next.js - `package.json`)

*   **`@supabase/supabase-js`**: Client library for Supabase authentication and real-time features.
*   **`zustand`**: For efficient global state management in the React frontend.
*   **`next`**: React framework for building the frontend application.
*   **`react`**, **`react-dom`**: Core React libraries.
*   **`tailwindcss`**, **`postcss`**, **`autoprefixer`**: For styling with utility-first CSS.
*   **`shadcn/ui`**: (Selected UI library) Provides accessible and customizable UI components.

### Key Integrations

*   **Gemini API (Google)**: Primary integration for AI-powered task analysis. All communication will be server-side (FastAPI).
*   **Supabase**: Continues to serve as the database, authentication provider, and real-time subscription engine.
*   **Vercel**: Deployment platform for the monorepo, handling both Next.js frontend and FastAPI serverless functions.

## Acceptance Criteria (Authoritative)

**Story 2.1: AI-Generated Smart Labels**

1.  Given I create or edit a task, when the backend saves the task, then it asynchronously calls the Gemini API with a structured prompt for label generation.
2.  Given the Gemini API returns valid labels, when the backend processes the response, then it populates the `labels` and `task_labels` tables in the database.
3.  Given the Gemini API call fails or returns invalid data, when the backend handles the error, then the task is still created successfully without any labels, and the error is logged.
4.  Given a task has labels, when the frontend displays the task, then the labels are shown as distinct visual elements (e.g., tags or badges).

**Story 2.2: AI-Suggested Priority Levels**

1.  Given I create or edit a task, when the backend saves the task, then it asynchronously calls the Gemini API with a structured prompt for priority suggestion.
2.  Given the Gemini API returns a valid priority level (e.g., an integer from 1 to 4), when the backend processes the response, then the `priority` field is updated on the `tasks` table.
3.  Given a task has a suggested priority, when the frontend displays the task, then the priority is clearly indicated (e.g., with a colored icon or border).

**Story 2.3: Filter Tasks by Smart Labels**

1.  Given I have tasks with AI-generated labels, when I click on a "work" label tag in the UI, then the frontend makes a `GET` request to `/api/v1/tasks?label=work`.
2.  Given the API returns a filtered list of tasks, when the UI updates, then only tasks with the "work" label are displayed.
3.  Given a filter is active, when I click a "Clear" or "All" button, then a `GET` request is made to `/api/v1/tasks` and all tasks are displayed again.

## Traceability Mapping

| Acceptance Criteria                                                                                    | Spec Section(s)            | Component(s)/API(s)                                   | Test Idea                                                                                                                   |
| :----------------------------------------------------------------------------------------------------- | :------------------------- | :---------------------------------------------------- | :-------------------------------------------------------------------------------------------------------------------------- |
| AC 2.1.1: Task creation/edit triggers async Gemini API call for label generation.                      | 4.3.1 (Workflow 1)         | `TaskService`, `AIService`, Gemini API                | Unit test `TaskService` triggers `AIService` asynchronously. Integration test `AIService` calls Gemini.                         |
| AC 2.1.2: Backend populates `labels` and `task_labels` tables upon valid Gemini response.              | 4.2.1, 4.3.1 (Workflow 1)  | `AIService`, `TaskRepository`, Database               | Integration test: Create task, verify labels and associations are correctly saved in DB.                                    |
| AC 2.1.3: Backend handles Gemini API failure, task still created without labels, error logged.         | 5.3 (Reliability)          | `AIService`, Logging                                  | Unit/Integration test: Mock Gemini API failure, check task creation (no labels) and error logs.                             |
| AC 2.1.4: Frontend displays task labels as distinct visual elements.                                   | 4.2.1 (Data Models)        | `LabelTag` component, `TaskRead` model, UI            | UI test: Verify labels are rendered correctly on task cards in various states (e.g., multiple labels, no labels).            |
| AC 2.2.1: Task creation/edit triggers async Gemini API call for priority suggestion.                   | 4.3.1 (Workflow 1)         | `TaskService`, `AIService`, Gemini API                | Unit test `TaskService` triggers `AIService` asynchronously. Integration test `AIService` calls Gemini for priority.        |
| AC 2.2.2: Backend updates `priority` field on `tasks` table upon valid Gemini response.                | 4.2.1, 4.3.1 (Workflow 1)  | `AIService`, `TaskRepository`, Database               | Integration test: Create task, verify `priority` field is updated correctly in DB.                                          |
| AC 2.2.3: Frontend displays task priority clearly.                                                     | 4.2.1 (Data Models)        | `TaskRead` model, UI Component                        | UI test: Verify priority (e.g., color-coded icon) is displayed correctly on task cards.                                     |
| AC 2.3.1: Frontend makes `GET /api/v1/tasks?label=work` when label tag clicked.                        | 4.2.3, 4.3.2 (Workflow 2)  | `LabelTag` component, `TaskStateService`, Frontend API | UI/E2E test: Click a label, intercept network request to verify correct API call with `label` query param.                 |
| AC 2.3.2: API returns filtered list of tasks; UI updates to show only tasks with "work" label.         | 4.2.3, 4.3.2 (Workflow 2)  | `TaskService`, `TaskRepository`, Database, Frontend   | Integration/E2E test: Verify only tasks with the selected label are displayed after filtering.                              |
| AC 2.3.3: Frontend makes `GET /api/v1/tasks` when "Clear" filter clicked, showing all tasks.           | 4.2.3, 4.3.2 (Workflow 2)  | `TaskStateService`, Frontend API                      | UI/E2E test: Click clear filter, verify network request to `/api/v1/tasks` (no label param), and all tasks are displayed. |

## Risks, Assumptions, Open Questions

*   **Risk:** Gemini API rate limits could be encountered with frequent task updates, leading to degraded user experience or service interruption.
    *   **Mitigation:** Implement client-side rate limiting on asynchronous AI calls, ensure proper backoff and retry logic in the `AIService`. Consider caching for highly repetitive task descriptions.
*   **Risk:** The quality and relevance of AI-generated labels and priority levels may vary, potentially requiring extensive prompt engineering and continuous refinement.
    *   **Mitigation:** Establish a clear feedback mechanism (internal or user-facing) to identify and act on sub-optimal AI outputs. Implement robust logging of prompts and responses for data-driven prompt optimization.
*   **Assumption:** The Gemini API will provide consistent response formats. Variations in the response structure could break parsing logic in the `AIService`.
    *   **Mitigation:** Implement flexible and defensive parsing logic in `AIService`, with clear error handling for unexpected response structures. Keep API client libraries up-to-date.
*   **Open Question:** What is the optimal context window or maximum length of task descriptions that the Gemini API can reliably process for label and priority generation without performance degradation or loss of accuracy?
    *   **Next Step:** Conduct performance and accuracy testing with varying task description lengths during early implementation.
*   **Open Question:** How will the growth of the `labels` table be managed? Will there be a need for label cleanup, merging of similar labels (e.g., "work" vs. "office"), or user-defined label hierarchies?
    *   **Next Step:** Monitor label proliferation post-deployment. This may inform a future epic for label management features.

## Test Strategy Summary

The testing strategy for Epic 2 will focus on ensuring the accuracy, reliability, and performance of the AI integration, alongside the correct functioning of new data models and API endpoints.

*   **Unit Tests:**
    *   **Backend (`pytest`):** Cover individual components like `TaskService` (ensuring correct invocation of `AIService`), `AIService` (mocking Gemini API to test prompt construction, response parsing, and error handling), and `TaskRepository` (verifying database interactions for tasks, labels, and `task_labels` creation/updates).
    *   **Frontend (`Jest`, `React Testing Library`):** Test `LabelTag` component rendering and click events, and `TaskStateService` logic for managing filtered task lists.
*   **Integration Tests:**
    *   **Backend API (`pytest`):** Verify `GET /api/v1/tasks?label={label}` returns correctly filtered results. Test `POST /api/v1/tasks` and `PUT /api/v1/tasks/{task_id}` to ensure end-to-end task saving, asynchronous AI analysis, and subsequent database updates.
    *   **Full Stack (`Playwright`):** Simulate user flows for creating/editing tasks, observing AI-generated labels and priorities appearing in the UI, and filtering tasks by clicking labels.
*   **AI-Specific Testing:**
    *   Develop a dedicated suite of tests for `AIService` that uses a mocked Gemini API to simulate various scenarios: successful responses (with diverse labels/priorities), API failures, and malformed responses. This ensures the service's resilience.
    *   Implement basic regression tests for AI output: Given a set of standard task inputs, verify that the AI consistently generates expected labels and priority ranges.
*   **Edge Case Testing:**
    *   Tasks with very short or very long descriptions.
    *   Tasks with ambiguous content (e.g., "call a friend" - personal or work?).
    *   Concurrent task creation/updates from multiple users.
    *   Scenarios where AI returns no labels or priority.
    *   Testing with empty label lists or non-existent labels in the filter endpoint.
*   **Performance Testing:** Load testing on the `/api/v1/tasks` endpoint with filtering enabled to ensure it meets latency requirements under expected load.
*   **Security Testing:** Verify that API keys are not exposed and that RLS policies prevent unauthorized access to task data.
